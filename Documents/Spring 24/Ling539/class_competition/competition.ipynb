{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e407c879-7d48-401a-b614-c6f006d6df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Iterable, List, Tuple, Text, Union\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2132705-8503-4d86-8b7d-c51177807697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An NDArray can either be a numpy array (np.ndarray) or a sparse matrix (spmatrix)\n",
    "NDArray = Union[np.ndarray, spmatrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2767e24d-0fcd-4991-8038-f4fa56e2f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path: Text) -> Iterator[Tuple[Text, Text, Text]]:\n",
    "    with open(data_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip the header row if present\n",
    "        for row in reader:\n",
    "            yield tuple(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc36a2f3-94d5-4374-a688-5937aefb00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_path: Text) -> Tuple[List[Text], List[Text]]:\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for _, text, label in read_data(data_path):\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3d42fb2-d9c3-438d-ab47-a0cf7d1d1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = read_data(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73e9ca27-ac6f-4907-b431-62a2f74f16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format is ID TEXT LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da9be076-3727-4a29-9ccf-614ea393c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToFeatures:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes an object for converting texts to features.    \n",
    "        \"\"\"\n",
    "        \n",
    "        # HINT: you may want to use a sklearn vectorizer. Be sure you've\n",
    "        #  worked through the sklearn tutorial in the course website and read\n",
    "        #  the documentation on sklearn's vectorizers at\n",
    "        # https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text\n",
    "        #\n",
    "        # Learning to use a library based only on reading its documentation\n",
    "        #  is one of the skills that we hope you'll learn in this course!\n",
    "        #\n",
    "        # If you take this approach, your code here will largely just be\n",
    "        #  calls to the corresponding methods of the sklearn vectorizer\n",
    "        #  that you choose. Don't forget to add the import statement\n",
    "        #  for your chosen vectorizer in the imports cell in the Assignment\n",
    "        #  Overview.\n",
    "        \n",
    "        self.vectorizer = CountVectorizer(\n",
    "              # case fold all text \n",
    "              # before generating n-grams\n",
    "              lowercase=True,\n",
    "              # optionally apply the specified function\n",
    "              # before counting n-grams\n",
    "              preprocessor=None,\n",
    "              # optionally provide a list of tokens to remove/ignore before generating n-grams\n",
    "              stop_words=None,\n",
    "              # specify a range of n-grams as (min_n, max_n). \n",
    "              # (1, 1) means unigrams.\n",
    "              # (1, 2) means unigrams and bigrams\n",
    "              # (4, 5) means 4-grams and 5-grams\n",
    "              ngram_range=(1, 2),\n",
    "              # \"word\", \"char\" (character), or \"char_wb\" n-grams\n",
    "              analyzer=\"word\",\n",
    "              # whether or not to use binary counts\n",
    "              binary=False\n",
    "            )\n",
    "\n",
    "    def fit(self, training_texts: Iterable[Text]) -> None:\n",
    "        \"\"\"\n",
    "        Fits (\"trains\") a TextToFeature instance on a collection of documents.\n",
    "        \n",
    "        The provided training texts are analyzed to determine the vocabulary, \n",
    "        i.e., all feature values that the converter will support. \n",
    "        Each such feature value will be associated with a unique integer index \n",
    "        that may later be accessed via the .index() method.\n",
    "\n",
    "        It is up to the implementer exactly what features to produce from a\n",
    "        text, but the features will always include some single words and some\n",
    "        multi-word expressions (e.g., \"need\" and \"to you\").\n",
    "        \n",
    "        \n",
    "        docs = [\n",
    "            \"LOL. is this u? http://supersketchyurl.com/dangerous\",\n",
    "            \"The IRS has been trying to reach you.\",\n",
    "            \"Enclosed is your Coyote Joe's Marketplace Rewards Card.\"\n",
    "            \"Logan I'd like to add you to my professional network on LinkedIn\",\n",
    "        ]\n",
    "        \n",
    "        t2f = TextToFeatures()\n",
    "        t2f.fit(docs)\n",
    "\n",
    "        :param training_texts: The training texts.\n",
    "        \"\"\"\n",
    "        self.vectorizer.fit(training_texts)\n",
    "\n",
    "        \n",
    "        \n",
    "    def index(self, feature: Text) -> Union[None, int]:\n",
    "        \"\"\"\n",
    "        Returns the index in the vocabulary of the given feature value.  \n",
    "        If the features isn't present, return None.\n",
    "\n",
    "        :param feature: A feature\n",
    "        :return: The unique integer index associated with the feature or None if not present.\n",
    "        \"\"\"\n",
    "        if feature not in self.vectorizer.vocabulary_:\n",
    "            return None\n",
    "        else:\n",
    "            return self.vectorizer.vocabulary_[feature]\n",
    "\n",
    "    def transform(self, texts: Iterable[Text]) -> NDArray:\n",
    "        \"\"\"\n",
    "        Creates a feature matrix from a sequence of texts.\n",
    "        \n",
    "        docs = [\n",
    "            \"LOL. is this u? http://supersketchyurl.com/dangerous\",\n",
    "            \"The IRS has been trying to reach you.\",\n",
    "            \"Enclosed is your Coyote Joe's Marketplace Rewards Card.\"\n",
    "            \"I'd like to add you to my professional network on LinkedIn\",\n",
    "        ]\n",
    "        \n",
    "        t2f = TextToFeatures()\n",
    "        t2f.fit(docs)\n",
    "\n",
    "        # this produces a NDArray representing our features for the provided doc\n",
    "        t2f.transform([\"Let's meet at Coyote Joe's at 6.\"])\n",
    "\n",
    "\n",
    "        Each row of the matrix corresponds to one of the input texts. The value\n",
    "        at index j of row i is the value in the ith text of the feature\n",
    "        associated with the unique integer j.\n",
    "\n",
    "        It is up to the implementer what the value of a feature that is present\n",
    "        in a text should be, though a common choice is 1. Features that are\n",
    "        absent from a text will have the value 0.\n",
    "\n",
    "        :param texts: A sequence of texts.\n",
    "        :return: A matrix, with one row of feature values for each text.\n",
    "        \"\"\"\n",
    "        return self.vectorizer.transform(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79d634d9-6dd4-4706-9b49-5a3708451994",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToLabels:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes an object for converting texts to labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        # HINT: As with the previous class, you may choose to use an sklearn\n",
    "        #  class here. See the documentation at\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "        #\n",
    "        # As before, be sure to add an import statement for the sklearn class\n",
    "        #  that you choose in the (editable) import cell towards the top of\n",
    "        #  this notebook.\n",
    "        \n",
    "        self.encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, training_labels: Iterable[Text]) -> None:\n",
    "        \"\"\"\n",
    "        Assigns each distinct label a unique integer.\n",
    "        \n",
    "        \n",
    "        Training labels are analyzed to determine the vocabulary, \n",
    "        i.e., all labels that the converter will support. \n",
    "        Each such label will be associated with a unique integer index \n",
    "        that may later be accessed via the .index() method.\n",
    "\n",
    "        :param training_labels: The training labels.\n",
    "        \"\"\"\n",
    "        self.encoder.fit(training_labels)\n",
    "        \n",
    "    def index(self, label: Text) -> Union[None, int]:\n",
    "        \"\"\"Returns the index in the vocabulary of the given label.\n",
    "\n",
    "        :param label: A label\n",
    "        :return: The unique integer index associated with the label.\n",
    "        \"\"\"\n",
    "        if label not in self.encoder.classes_:\n",
    "            return None\n",
    "        else:\n",
    "            return np.where(self.encoder.classes_ == label)[0][0]\n",
    "\n",
    "    def transform(self, labels: Iterable[Text]) -> NDArray:\n",
    "        \"\"\"\n",
    "        Creates a label vector from a sequence of labels.\n",
    "\n",
    "        Each entry in the vector corresponds to one of the input labels. The\n",
    "        value at index j is the unique integer associated with the jth label.\n",
    "\n",
    "        :param labels: A sequence of labels.\n",
    "        :return: A vector, with one entry for each label.\n",
    "        \"\"\"\n",
    "        return self.encoder.transform(labels)\n",
    "        \n",
    "        \n",
    "    def __contains__(self, label: Text) -> bool:\n",
    "        \"\"\"\n",
    "        Special \"dunder\" method to check if a label is known to the TextToLabels instance.\n",
    "        \n",
    "        labeler = TextToLabels()\n",
    "        labeler.fit([\"POSITIVE\", \"NEGATIVE\"])\n",
    "\n",
    "        # should be True:\n",
    "        \"POSITIVE\" in labeler \n",
    "        \n",
    "        # should be False:\n",
    "        \"MBOP\" in labeler\n",
    "        \n",
    "        :return: True if the label was seen in the training data; False otherwise\n",
    "        \"\"\"\n",
    "        # NOTE: you do not need to change this if you've implemented .index() correctly!\n",
    "        return False if self.index(label) is None else True\n",
    "\n",
    "    def inverse_transform(self, labels: NDArray) -> Iterable[Text]:\n",
    "        return self.encoder.inverse_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72d933be-297a-4cb2-8b8d-df54920555d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initalizes a logistic regression classifier.\n",
    "        \"\"\"\n",
    "\n",
    "        # HINT: See the documentation at\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "        #  Think about the input parameter values you'll need to use.\n",
    "        #\n",
    "        # As before, be sure to add an import statement for the sklearn class\n",
    "        #  that you choose in the (editable) import cell towards the top of\n",
    "        #  this notebook.\n",
    "        \n",
    "        self.classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "\n",
    "    def train(self, features: NDArray, labels: NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Trains the classifier using the given training examples.\n",
    "\n",
    "        :param features: A feature matrix, where each row represents a text.\n",
    "        Such matrices will typically be generated via TextToFeatures.\n",
    "        :param labels: A label vector, where each entry represents a label.\n",
    "        Such vectors will typically be generated via TextToLabels.\n",
    "        \"\"\"\n",
    "        self.classifier.fit(features, labels)\n",
    "    \n",
    "    # just an alias for \"train\"\n",
    "    #fit = train\n",
    "    \n",
    "    def predict(self, features: NDArray) -> NDArray:\n",
    "        \"\"\"Makes predictions for each of the given examples.\n",
    "\n",
    "        :param features: A feature matrix, where each row represents a text.\n",
    "        Such matrices will typically be generated via TextToFeatures.\n",
    "        :return: A prediction vector, where each entry represents a label.\n",
    "        \"\"\"\n",
    "        return self.classifier.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ba5ed67-c484-4b50-badc-6d94ed5e5579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70317\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(training_texts))\n",
    "print(len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a4be941-9363-4d29-a2c5-e0b8409cca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_predict(train_path: Text, test_path: Text) -> List[Tuple[Text, Text]]:\n",
    "    training_texts, training_labels = prepare_data(train_path)\n",
    "    feature_converter = TextToFeatures()\n",
    "    label_converter = TextToLabels()\n",
    "    feature_converter.fit(training_texts)\n",
    "    label_converter.fit(training_labels)\n",
    "\n",
    "    training_features = feature_converter.transform(training_texts)\n",
    "    training_numeric_labels = label_converter.transform(training_labels)\n",
    "\n",
    "    clf = Classifier()\n",
    "    clf.train(training_features, training_numeric_labels)\n",
    "\n",
    "    test_ids = []\n",
    "    test_texts = []\n",
    "    for row in read_data(test_path):\n",
    "        test_id, text = row[0], row[1]\n",
    "        test_ids.append(test_id)\n",
    "        test_texts.append(text)\n",
    "\n",
    "    test_features = feature_converter.transform(test_texts)\n",
    "    predicted_labels = clf.predict(test_features)\n",
    "    predicted_labels_text = label_converter.inverse_transform(predicted_labels)\n",
    "\n",
    "    return list(zip(test_ids, predicted_labels_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "334dab64-51ca-4748-a1c6-7d988a8e6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = process_and_predict(\"data/train.csv\", \"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65528166-dd15-4f44-aa6e-7a3ad5dab8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/attempt2.csv\", 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['ID', 'LABEL'])  # Writing the header\n",
    "        writer.writerows(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef847342-bf9e-4588-b6b0-2c84e0089e34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
